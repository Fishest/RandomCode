{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Online training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !cat /project/python/lib/ml/cnnscore.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import print_function, absolute_import\n",
    "\n",
    "import os\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from tseries.chartdata import get_range\n",
    "import datetime\n",
    "from database import db_session\n",
    "from models import Algo\n",
    "\n",
    "import chainer\n",
    "from chainer import optimizers\n",
    "# from chainer import serializers\n",
    "from chainer import Variable\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "\n",
    "from trading import tm\n",
    "\n",
    "\n",
    "def _get_algo_data(algo_id):\n",
    "    \"\"\"Get paramaters from an algorithm_id.\"\"\"\n",
    "    try:\n",
    "        al = db_session.query(Algo).get(algo_id)\n",
    "        st = al.strategy\n",
    "        mp = st.model_params\n",
    "        dataset = al.model.dataset[\"data\"]\n",
    "        sc = al.score\n",
    "        start = sc.algo.current_backtest.start\n",
    "        end = sc.algo.current_backtest.end\n",
    "    finally:\n",
    "        db_session.remove()\n",
    "    return mp, dataset, sc, start, end\n",
    "\n",
    "\n",
    "def _center_01(x):\n",
    "    \"\"\"Center data between 0 and 1.\"\"\"\n",
    "    c = (x - x.min()) / (x.max() - x.min())\n",
    "    return np.array([i for i in c]).astype(np.float32)\n",
    "\n",
    "\n",
    "def _discard_grad_and_normalizers(mp):\n",
    "    \"\"\"Discard gradients and normalizers from model parameters.\"\"\"\n",
    "    features = []\n",
    "    for idxi, i in enumerate(mp[\"features\"]):\n",
    "        if \"_dg2\" in i[\"id\"]:\n",
    "            continue\n",
    "        else:\n",
    "            tmp = i\n",
    "            tmp[u\"normalizers\"] = []\n",
    "            features.append(tmp)\n",
    "            if tmp[\"indicator_type\"] == \"DelayedGrad\":\n",
    "                tmp[\"indicator_type\"] = \"Identity\"\n",
    "            tmp.pop(\"scale\", None)\n",
    "    mp[\"features\"] = features\n",
    "\n",
    "\n",
    "def _get_indicators_group(mp):\n",
    "    \"\"\"Group similar indicators.\n",
    "\n",
    "    Group indicators by names, and keep related ones together,\n",
    "    ex. \"MACD\" and \"MACD-signal\".\n",
    "    \"\"\"\n",
    "\n",
    "    scaling_list = {\"Function\": [], \"Identity\": [], \"MA\": [], \"MACD\": [],\n",
    "                    \"StdDev\": [], \"BBANDS\": [], \"RSI\": [], \"UMustache\": [],\n",
    "                    \"LMustache\": [], \"CandleSize\": [], \"PivotPoint\": [],\n",
    "                    \"PlusDI\": [], \"MinusDI\": [], \"ADX\": [], \"Momentum\": [],\n",
    "                    \"Stochastic\": [], \"SMI\": [], \"Operator\": [],\n",
    "                    \"Ichimoku\": [], \"HLBand\": [], \"RCI\": [], \"DiffM\": [],\n",
    "                    \"Mustache\": [],  # added\n",
    "                    }\n",
    "    _linked_ohlc = [\"BBANDS\", \"HLBand\"]\n",
    "\n",
    "    for idxi, i in enumerate(mp[\"features\"]):\n",
    "        try:\n",
    "            t = i[\"indicator_type\"]\n",
    "            if t == \"Identity\" or t == \"DelayedGrad\" or i in _linked_ohlc:\n",
    "                scaling_list[\"Identity\"].append(idxi)\n",
    "            elif \"Mustache\" in t:\n",
    "                scaling_list[\"Mustache\"].append(idxi)\n",
    "            elif t == \"MACD\":\n",
    "                if \"-signal\" not in i[\"id\"]:\n",
    "                    scaling_list[i[\"id\"]] = [idxi]\n",
    "                else:\n",
    "                    scaling_list[i[\"id\"][:-7]].append(idxi)\n",
    "            else:\n",
    "                scaling_list[t].append(idxi)\n",
    "        except KeyError:\n",
    "            scaling_list[t] = [idxi]\n",
    "\n",
    "    indicators_groups = []\n",
    "    for i in scaling_list.values():\n",
    "        if i != []:\n",
    "            indicators_groups.append(i)\n",
    "    return indicators_groups\n",
    "\n",
    "\n",
    "def _check_negative_inputs(dataset):\n",
    "    if any([i[\"label\"] == ['neg_icon'] for i in dataset]):\n",
    "        raise NotImplementedError(\"Can't handle negative inputs yet\")\n",
    "\n",
    "\n",
    "def _load_df_range(mp, indicators, start, end):\n",
    "    \"\"\"Load dataframe to work on.\n",
    "\n",
    "    Add +-datetime.timedelta(15).\n",
    "    \"\"\"\n",
    "\n",
    "    d_start = start\n",
    "    d_end = end\n",
    "    d_start -= datetime.timedelta(15)\n",
    "    d_end += datetime.timedelta(15)\n",
    "\n",
    "    # get params\n",
    "    symbol = mp['symbol']\n",
    "    timeframe = mp['timeframe']\n",
    "    indi = mp[\"features\"]\n",
    "\n",
    "    r = get_range(d_start, d_end, symbol, timeframe, indi)\n",
    "\n",
    "    # correct index\n",
    "    date = r.pop(\"Date\", None)\n",
    "    df = pd.DataFrame.from_dict(r).astype(np.float32)\n",
    "    df[\"Date\"] = date\n",
    "    df[\"Date\"] = df[\"Date\"].apply(\n",
    "        lambda x: tm.T(x / 1e9))  # rescale\n",
    "    df = df.set_index(\"Date\")\n",
    "\n",
    "    # correct columns name\n",
    "    df = df[[\"Open\", \"High\", \"Low\", \"Close\"] + indicators[4:]]\n",
    "    return df\n",
    "\n",
    "\n",
    "def _easing(t, b, c, d):\n",
    "    t /= d\n",
    "    return c * t * t + b\n",
    "\n",
    "\n",
    "def _process_range(r, df, align, justify_right=True):\n",
    "    \"\"\"Process a range and align it by right side.\"\"\"\n",
    "\n",
    "    startidx = df.index.get_loc(tm.T(r[\"start\"]), method=\"ffill\")\n",
    "    endidx = df.index.get_loc(tm.T(r[\"end\"]), method=\"bfill\")\n",
    "\n",
    "    datalen = endidx - startidx\n",
    "    if datalen >= align:\n",
    "        startidx = endidx - align + 1\n",
    "        datalen = endidx - startidx\n",
    "    assert (datalen < align)\n",
    "\n",
    "    if justify_right:\n",
    "        leftmost = endidx - align\n",
    "        rightmost = endidx\n",
    "        start_margin = datalen\n",
    "    else:\n",
    "        start_margin = (align - datalen) / 2\n",
    "        leftmost = max(startidx - start_margin, 0)\n",
    "        rightmost = min(leftmost + align, df.shape[0])\n",
    "\n",
    "        if rightmost - leftmost < align:\n",
    "            # shift back the leftmost point\n",
    "            leftmost -= align - (rightmost - leftmost)\n",
    "\n",
    "    assert (leftmost >= 0)\n",
    "    assert (rightmost <= df.shape[0])\n",
    "    assert (rightmost - leftmost == align)\n",
    "\n",
    "    mini_df = df[leftmost:rightmost]\n",
    "    x_values = mini_df\n",
    "\n",
    "    d = datalen\n",
    "    t = np.arange(1, d + 1, dtype=np.float32)\n",
    "    y = _easing(t, 0.0, t / d, d)\n",
    "    y_values = np.zeros(align, dtype=np.float32)\n",
    "    if justify_right:\n",
    "        y_values[-datalen:] = y\n",
    "    else:\n",
    "        y_values[start_margin:start_margin + datalen] = y\n",
    "\n",
    "    return x_values.astype(np.float32), y_values\n",
    "\n",
    "\n",
    "def _select_normalize_from_df(r, df, align, indicators_groups,\n",
    "                              justify_right=True):\n",
    "    \"\"\"Process range and normalize it.\"\"\"\n",
    "\n",
    "    t, y = _process_range(r, df, align, justify_right=justify_right)\n",
    "\n",
    "    c = t.values\n",
    "    for i in indicators_groups:\n",
    "        c[:, i] = _center_01(c[:, i])\n",
    "    return c.astype(np.float32), y\n",
    "\n",
    "\n",
    "def _get_output_size(*args):\n",
    "    \"\"\"Compute input size for fully connected layer after CNN.\n",
    "\n",
    "    Args:\n",
    "        *args: (in,\n",
    "        [l_pad, l_k, l_ch, l_st],\n",
    "        [l_pad, l_k, l_ch, l_st],\n",
    "        ...)\n",
    "\n",
    "    Returns:\n",
    "        size (int)\n",
    "\n",
    "    Example:\n",
    "        (input_size,\n",
    "         [layer_1_padding, layer_1_filter, layer_1_channel, layer_1_stride],\n",
    "         [layer_2_padding, layer_2_filter, layer_2_channel, layer_2_stride],\n",
    "         ...\n",
    "         [layer_N_padding, layer_N_filter, layer_N_channel, layer_N_stride],\n",
    "        )\n",
    "\n",
    "    Computed as:\n",
    "    $$n_{out}=((n_{in} + 2*n_{padding} - n_{filter})\\\n",
    "/ n_{stride} + 1) * n_{channel}$$\n",
    "    \"\"\"\n",
    "    out = -1\n",
    "    previous = 1\n",
    "    for idx, i in enumerate(args):\n",
    "        if idx == 0:\n",
    "            out = i\n",
    "            continue\n",
    "        out = ((out / previous + 2 * i[0] - i[1]) / i[3] + 1) * i[2]\n",
    "        previous = i[2]\n",
    "    return out\n",
    "\n",
    "\n",
    "class _CNN_1D(chainer.Chain):\n",
    "    \"\"\"1D Convolutional Neural Network.\n",
    "\n",
    "    Use batch normalization and dropout.\n",
    "    3 CNN layers, 2 FC layers\n",
    "    Arbitrary hyperparameters, empirically decided.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, input_channels):\n",
    "        # hyper parameters\n",
    "        ch1, ch2, ch3 = 32, 64, 128\n",
    "        k1, k2, k3 = 11,  5,   3\n",
    "        st1, st2, st3 = 3,  2,   1\n",
    "        p1, p2, p3 = 2,  2,   2\n",
    "\n",
    "        # parameters\n",
    "        self.input_size = input_size\n",
    "        self.input_channels = input_channels\n",
    "\n",
    "        self.fullco_val = _get_output_size(self.input_size,\n",
    "                                           [p1, k1, ch1, st1],\n",
    "                                           [p2, k2, ch2, st2],\n",
    "                                           [p3, k3, ch3, st3],\n",
    "                                           )\n",
    "\n",
    "        super(_CNN_1D, self).__init__(\n",
    "            c1_1=L.Convolution2D(self.input_channels, ch1,\n",
    "                                 (k1, 1), stride=(st1, 1),\n",
    "                                 pad=(p1, 0)),\n",
    "            bn1_1=L.BatchNormalization(ch1),\n",
    "\n",
    "            c2_1=L.Convolution2D(\n",
    "                ch1, ch2, (k2, 1), stride=(st2, 1), pad=(p2, 0)),\n",
    "            bn2_1=L.BatchNormalization(ch2),\n",
    "\n",
    "            c3_1=L.Convolution2D(\n",
    "                ch2, ch3, (k3, 1), stride=(st3, 1), pad=(p3, 0)),\n",
    "            bn3_1=L.BatchNormalization(ch3),\n",
    "\n",
    "            # The problem with this fully connected layer is that it's\n",
    "            # preventing the network to work on arbitrary sized inputs...\n",
    "            #\n",
    "            # Use global average pooling to get rid of this!\n",
    "            # http://image-net.org/challenges/LSVRC/\n",
    "            # 2014/slides/ILSVRC2014_NUS_release.pdf    slide 6\n",
    "            # https://arxiv.org/pdf/1312.4400.pdf\n",
    "            # http://cnnlocalization.csail.mit.edu/\n",
    "            #\n",
    "            l4=L.Linear(self.fullco_val, 200),\n",
    "            l5=L.Linear(200, 1),\n",
    "        )\n",
    "\n",
    "    def __call__(self, x, train=False):\n",
    "        h = F.relu(self.bn1_1(self.c1_1(x), test=not train))\n",
    "        h = F.dropout(h, ratio=0.5, train=train)\n",
    "\n",
    "        h = F.relu(self.bn2_1(self.c2_1(h), test=not train))\n",
    "        h = F.dropout(h, ratio=0.5, train=train)\n",
    "\n",
    "        h = F.relu(self.bn3_1(self.c3_1(h), test=not train))\n",
    "        h = F.dropout(h, ratio=0.5, train=train)\n",
    "\n",
    "        h = F.dropout(F.relu(self.l4(h)), ratio=0.5, train=train)\n",
    "        h = self.l5(h)  # can add a F.tanh if we want to restrain it to -1,+1\n",
    "        return h\n",
    "\n",
    "\n",
    "def _prepare_samples_CNN(postive_data, negative_data, indicators_groups,\n",
    "                         augment_factor=1, asymetric_augment=1):\n",
    "    \"\"\"Reshape samples to be fed into the CNN.\"\"\"\n",
    "\n",
    "    # selected_data.shape[0]*2 #10000\n",
    "    size_dataset = postive_data.shape[0] * augment_factor * 2\n",
    "    len_samples = postive_data.shape[1]\n",
    "\n",
    "    # Trivial negative data\n",
    "    # fake_data = center(np.cumsum(np.random.randn(size_dataset/2,\n",
    "    # len_samples),axis=1).astype(np.float32)) #*2 - 1\n",
    "    # fake_data = np.array([\n",
    "    #         center(i) for i in np.cumsum(np.random.randn(\n",
    "    # size_dataset/2, len_samples),axis=1).astype(np.float32)\n",
    "    #     ])\n",
    "\n",
    "    # Randomly sampled and normalized negative data\n",
    "    negative_samples = np.empty(\n",
    "        (size_dataset / 2 * asymetric_augment, len_samples,\n",
    "         postive_data.shape[2]), dtype=np.float32)\n",
    "    for idxi, i in enumerate(negative_samples):\n",
    "        rand = np.random.randint(0, len(negative_data) - (len_samples))\n",
    "        c = copy.deepcopy(negative_data[rand:rand + len_samples])\n",
    "        for i in indicators_groups:\n",
    "            c[:, i] = _center_01(c[:, i])\n",
    "\n",
    "        negative_samples[idxi] = c\n",
    "\n",
    "    if augment_factor > 1:\n",
    "        positive_samples = np.repeat(postive_data, augment_factor,\n",
    "                                     axis=0)\n",
    "    else:\n",
    "        positive_samples = postive_data\n",
    "\n",
    "    # Reshape data for convenient use with CNN\n",
    "    data = np.concatenate(\n",
    "        (positive_samples, negative_samples)).astype(np.float32)\n",
    "    labels = np.array([[1]] * len(positive_samples) + [[-1]]\n",
    "                      * len(negative_samples)).astype(np.int32)\n",
    "    data = np.swapaxes(data, 1, 2)\n",
    "    data = np.reshape(data, data.shape + (1,))\n",
    "\n",
    "    permut = np.random.permutation(len(data))\n",
    "    data = data[permut]\n",
    "    labels = labels[permut]\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "class _train_CNN(chainer.Chain):\n",
    "    \"\"\"Trainer encapsuling CNN with MSE.\"\"\"\n",
    "\n",
    "    def __init__(self, predictor):\n",
    "        super(_train_CNN, self).__init__(predictor=predictor)\n",
    "\n",
    "    def __call__(self, x, t, train=True):\n",
    "        y = self.predictor(x, train=train)\n",
    "        self.loss = F.mean_squared_error(y, t)\n",
    "        return self.loss\n",
    "\n",
    "\n",
    "def _eval_net(network, x, y, eval_on=10, batch_size=400, gpu_id=None):\n",
    "    \"\"\"Evaluate network performance on a subset.\"\"\"\n",
    "\n",
    "    if gpu_id is None:\n",
    "        gpu_id = int(os.environ.get('GPU_ID', 0))\n",
    "\n",
    "    # true/false positive, true/false negative\n",
    "    tp, fp, tn, fn = 0, 0, 0, 0\n",
    "\n",
    "    N = x.shape[0]\n",
    "\n",
    "    # Random permutations, permissive for N < batch_size\n",
    "    perm = np.random.permutation(max(N, (batch_size / N + 1) * N)) % N\n",
    "    np.random.shuffle(perm)\n",
    "    perm = perm[:max(N, batch_size)]\n",
    "\n",
    "    b = 0 if batch_size >= N else np.random.randint(0, N - batch_size)\n",
    "\n",
    "    _examples = x[perm[b:b + batch_size]]\n",
    "    _labels = y[perm[b:b + batch_size]]\n",
    "\n",
    "    examples = Variable(_examples)\n",
    "    examples.to_gpu(gpu_id)\n",
    "\n",
    "    pred = network(examples, train=False)\n",
    "    pred.to_cpu()\n",
    "\n",
    "    for idxi, i in enumerate(_labels):\n",
    "        if np.sign(pred.data[idxi, 0]) > 0:\n",
    "            if np.sign(_labels[idxi]) == np.sign(pred.data[idxi, 0]):\n",
    "                # true positive\n",
    "                tp += 1\n",
    "            else:\n",
    "                # false positive\n",
    "                fp += 1\n",
    "        else:\n",
    "            if np.sign(_labels[idxi]) == np.sign(pred.data[idxi, 0]):\n",
    "                # true negative\n",
    "                tn += 1\n",
    "            else:\n",
    "                # false negative\n",
    "                fn += 1\n",
    "\n",
    "    eval_on = float(batch_size)\n",
    "    mean_loss = (tp + tn) / eval_on\n",
    "    precision, recall = 0 if tp + fp == 0 else tp / \\\n",
    "        float(tp + fp), 0 if tp + fn == 0 else tp / float(tp + fn)\n",
    "    confusion_matrix = [tp / eval_on,\n",
    "                        fp / eval_on,\n",
    "                        fn / eval_on,\n",
    "                        tn / eval_on]\n",
    "    f1_score = 0 if precision + recall == 0 else 2 * \\\n",
    "        ((precision * recall) / (precision + recall))  # harmonic mean\n",
    "    return mean_loss, precision, recall, f1_score, confusion_matrix\n",
    "\n",
    "\n",
    "def _gpu_trainer(network, trainer, optimizer,\n",
    "                 x_train, x_test, y_train, y_test,\n",
    "                 n_steps=100, batch_size=400,\n",
    "                 log=False, display=False, print_nth=10, gpu_id=None):\n",
    "    \"\"\"Convenience function to train neural network on GPU.\n",
    "\n",
    "    Train a neural network on GPU, eventually log and print progress.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        network     (_CNN_1D): neural network.\n",
    "        trainer  (_train_CNN): network trainer.\n",
    "        optimizer   (chainer): optimizer.\n",
    "        x_train (numpy.array): training data.\n",
    "        x_test  (numpy.array): testing data, may be null.\n",
    "        y_train (numpy.array): training labels.\n",
    "        y_test  (numpy.array): testing labels, may be null.\n",
    "        n_steps         (int): number of training steps, default 100.\n",
    "        batch_size      (int): batch size, default 400.\n",
    "        log            (bool): log progress, default False.\n",
    "        display        (bool): display progress, default False.\n",
    "        print_nth       (int): display progress every nth, default 10.\n",
    "        gpu_id          (int): GPU id.\n",
    "\n",
    "    Returns:\n",
    "        If `log`, return evaluation results on train and test,\n",
    "        and confusion matrix on train and test:\n",
    "        `log_train, log_test, cm, cm_`\n",
    "        otherwirse:\n",
    "        `None, None, None, None`.\n",
    "    \"\"\"\n",
    "\n",
    "    if gpu_id is None:\n",
    "        gpu_id = int(os.environ.get('GPU_ID', 0))\n",
    "\n",
    "    network.to_gpu(gpu_id)\n",
    "    trainer.to_gpu(gpu_id)\n",
    "\n",
    "    N = x_train.shape[0]\n",
    "\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    if log:\n",
    "        if display:\n",
    "            logger.debug(\"> Training for %d steps\" % n_steps)\n",
    "            logger.debug(\"> Training samples: %d. Testing samples: %d\\n\" % (\n",
    "                len(y_train), len(y_test)))\n",
    "            logger.debug(\"          (Accuracy) train , test   || \\\n",
    "(Precison/Recall) train , test ||  (F1_score) train , test\")\n",
    "            logger.debug(\"-\" * 100)\n",
    "        print_nth = n_steps if (print_nth > n_steps) or (\n",
    "            print_nth == -1) else print_nth\n",
    "        p_every = n_steps / print_nth\n",
    "        l = len(range(0, n_steps - 1, p_every))\n",
    "        log_train, log_test = np.zeros((l + 1, 5)), np.zeros((l + 1, 5))\n",
    "        log_idx = 0\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        network.zerograds()\n",
    "\n",
    "        # Random permutations, permissive for N < batch_size\n",
    "        perm = np.random.permutation(max(N, (batch_size / N + 1) * N)) % N\n",
    "        np.random.shuffle(perm)\n",
    "        perm = perm[:max(N, batch_size)]\n",
    "\n",
    "        b = 0 if batch_size >= N else np.random.randint(0, N - batch_size)\n",
    "\n",
    "        _examples = x_train[perm[b:b + batch_size]]\n",
    "        _labels = y_train[perm[b:b + batch_size]]\n",
    "\n",
    "        examples = Variable(_examples)\n",
    "        labels = Variable(_labels.astype(np.float32))\n",
    "        examples.to_gpu(gpu_id)\n",
    "        labels.to_gpu(gpu_id)\n",
    "\n",
    "        optimizer.update(trainer, examples, labels)\n",
    "\n",
    "        if log and (step % p_every == 0 or step == n_steps - 1):\n",
    "            lo, pr, re, f1, cm = _eval_net(\n",
    "                network, x_train, y_train, eval_on=1, batch_size=400,\n",
    "                gpu_id=gpu_id)\n",
    "            lo_, pr_, re_, f1_, cm_ = _eval_net(\n",
    "                network, x_test, y_test, eval_on=1, batch_size=400,\n",
    "                gpu_id=gpu_id)\n",
    "            log_train[log_idx] = np.array([step, lo, pr, re, f1])\n",
    "            log_test[log_idx] = np.array([step, lo_, pr_, re_, f1_])\n",
    "            log_idx += 1\n",
    "            if display:\n",
    "                logger.debug(\"Step %s:        %0.2f , %0.2f   ||    \\\n",
    "%0.2f/%0.2f , %0.2f/%0.2f      ||       %0.2f , %0.2f\"\n",
    "                             % (str(step).ljust(8),\n",
    "                                lo, lo_, pr, re,\n",
    "                                pr_, re_, f1, f1_))\n",
    "\n",
    "    if log:\n",
    "        return log_train, log_test, cm, cm_\n",
    "    else:\n",
    "        return None, None, None, None\n",
    "\n",
    "\n",
    "def _split_train_test(ratio, data, labels):\n",
    "    \"\"\"Split between train and test.\"\"\"\n",
    "    assert data.shape[0] == labels.shape[0]\n",
    "    l = int(len(data) * ratio)\n",
    "    x_train, x_test = data[:l], data[l:]\n",
    "    y_train, y_test = labels[:l], labels[l:]\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "def _rolling_window_lastaxis(a, window):\n",
    "    \"\"\"Rollout an array on one axis.\"\"\"\n",
    "    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n",
    "    strides = a.strides + (a.strides[-1],)\n",
    "    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n",
    "\n",
    "\n",
    "def _rolling_window(a, window):\n",
    "    \"\"\"Rollout an array on multiple axis.\"\"\"\n",
    "    for i, win in enumerate(window):\n",
    "        if win > 1:\n",
    "            a = a.swapaxes(i, -1)\n",
    "            a = _rolling_window_lastaxis(a, win)\n",
    "            a = a.swapaxes(-2, i)\n",
    "    return a\n",
    "\n",
    "\n",
    "def _normalize_range(df, indicators_groups, window_size):\n",
    "    \"\"\"Normalize a Dataframe while preserving groups relations.\"\"\"\n",
    "\n",
    "    data = df.values\n",
    "\n",
    "    indicators_size = [len(i) for i in sorted(indicators_groups)]\n",
    "    indicators_loc = np.r_[0, np.cumsum(indicators_size)[:-1]]\n",
    "\n",
    "    # expand data over a rolling window\n",
    "    rolled = _rolling_window(data, (window_size, data.shape[1]))\n",
    "    rolled = np.reshape(\n",
    "        rolled, (rolled.shape[0], window_size, rolled.shape[-1]))\n",
    "\n",
    "    # maximum value over each window for each group\n",
    "    _max = np.maximum.reduceat(np.max(rolled, axis=1), indicators_loc, axis=1)\n",
    "    r_max = np.repeat(_max, indicators_size, axis=1)\n",
    "\n",
    "    # minimum value over each window for each group\n",
    "    _min = np.minimum.reduceat(np.min(rolled, axis=1), indicators_loc, axis=1)\n",
    "    r_min = np.repeat(_min, indicators_size, axis=1)\n",
    "\n",
    "    # normalize between 0 and 1\n",
    "    difference = r_max - r_min\n",
    "    substracted = rolled - r_min[:, np.newaxis, :]\n",
    "    divided = substracted / difference[:, np.newaxis, :]\n",
    "\n",
    "    # consistane shape\n",
    "    divided = np.swapaxes(divided, 1, 2)\n",
    "    return np.reshape(divided, divided.shape + (1,))\n",
    "\n",
    "\n",
    "def _output(cnn, arr, gpu_id=None):\n",
    "    \"\"\"Compute a batch of forward pass.\"\"\"\n",
    "\n",
    "    if gpu_id is None:\n",
    "        gpu_id = int(os.environ.get('GPU_ID', 0))\n",
    "\n",
    "    example = Variable(arr, volatile=True)\n",
    "    example.to_gpu(gpu_id)\n",
    "\n",
    "    pred = cnn(example, train=False)\n",
    "    pred.to_cpu()\n",
    "    return pred.data\n",
    "\n",
    "\n",
    "def train(*args, **kwargs):\n",
    "    \"\"\"Empty\"\"\"\n",
    "    return\n",
    "\n",
    "\n",
    "def calc_score(algo_id, gpu_id=None):\n",
    "    \"\"\"Compute CNN scores.\n",
    "\n",
    "    Train a CNN network and predict scores.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        algo_id         (int): algorithm id.\n",
    "        gpu_id          (int): GPU id, optional, default: `env['GPU_ID']`.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame indexed by date with a confidence value.\n",
    "        For example:\n",
    "\n",
    "        Date                | confidence\n",
    "        --------------------+-------------\n",
    "        2015-08-25 07:45:00 | 0.000000\n",
    "        2015-08-25 07:50:00 | 0.666309\n",
    "        2015-08-25 07:55:00 | 0.711061\n",
    "\n",
    "    Example:\n",
    "        `calc_score(42)`\n",
    "    \"\"\"\n",
    "\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    if gpu_id is None:\n",
    "        gpu_id = int(os.environ.get('GPU_ID', 0))\n",
    "\n",
    "    # local parameters\n",
    "    align = 25            # candles to take before entry point\n",
    "    gpu_batch_size = 400  # max batch size to match GPU\n",
    "    seed = 1337           # random seed\n",
    "\n",
    "    mp, dataset, sc, bt_start, bt_end = _get_algo_data(algo_id)\n",
    "\n",
    "    _discard_grad_and_normalizers(mp)\n",
    "\n",
    "    indicators = [str(i[\"id\"]) for i in mp[\"features\"]]\n",
    "    indicators_groups = _get_indicators_group(mp)\n",
    "\n",
    "    # negative inputs are not handled yet\n",
    "    # _check_negative_inputs(dataset)\n",
    "    dataset = [i for i in dataset if i['label'] == []]\n",
    "\n",
    "    # ensure to load everything we will need\n",
    "    ds_start = tm.T(dataset[0][\"start\"]).to_pydatetime()\n",
    "    ds_end = tm.T(dataset[-1][\"end\"]).to_pydatetime()\n",
    "    _df_start = min(ds_start, bt_start)\n",
    "    _df_end = max(ds_end, bt_end)\n",
    "\n",
    "    # get the DataFrame corresponding to dataset range plus margins\n",
    "    # **high time consuming step**\n",
    "    df = _load_df_range(mp, indicators, start=_df_start, end=_df_end)\n",
    "\n",
    "    # get positive samples from the dataset\n",
    "    x_and_y = zip(*[_select_normalize_from_df(sample,\n",
    "                                              df,\n",
    "                                              align,\n",
    "                                              indicators_groups,\n",
    "                                              justify_right=True\n",
    "                                              ) for sample in dataset])\n",
    "    selected_data, labels = np.array(x_and_y[0]), np.array(x_and_y[1])\n",
    "\n",
    "    # get random negative samples\n",
    "    _align = min(100000, len(df))\n",
    "    tmp, _ = _select_normalize_from_df({u'end': u'2005/01/01 17:45',\n",
    "                                        u'start': u'2018/12/31 15:00'},\n",
    "                                       df, _align, indicators_groups,\n",
    "                                       justify_right=False)\n",
    "    data_indicators = np.array(tmp)\n",
    "\n",
    "    # prepare the data to feed into the neural network\n",
    "    samples, labels = _prepare_samples_CNN(selected_data, data_indicators,\n",
    "                                           indicators_groups,\n",
    "                                           augment_factor=1,\n",
    "                                           asymetric_augment=1)\n",
    "\n",
    "    # splitting with a ratio=1, no validation data\n",
    "    x_train, x_test, y_train, y_test = _split_train_test(1, samples, labels)\n",
    "\n",
    "    logger.debug(\"done preparing training data\")\n",
    "\n",
    "    # fixed random seed\n",
    "    with chainer.cuda.get_device(gpu_id):\n",
    "        chainer.cuda.cupy.random.seed(seed=seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # setup CNN\n",
    "    cnn = _CNN_1D(input_size=selected_data.shape[1],\n",
    "                  input_channels=selected_data.shape[2])\n",
    "    t_cnn = _train_CNN(cnn)\n",
    "    optimizer = optimizers.Adam()\n",
    "    optimizer.setup(t_cnn)\n",
    "\n",
    "    # train CNN on GPU\n",
    "    log_train, log_test, cm, cm_ = _gpu_trainer(\n",
    "        cnn, t_cnn, optimizer, x_train, x_test, y_train, y_test,\n",
    "        n_steps=100, batch_size=gpu_batch_size,\n",
    "        log=False, display=False, print_nth=25, gpu_id=gpu_id)\n",
    "    logger.debug(\"done training CNN\")\n",
    "\n",
    "    # normalize the full range to process\n",
    "    # **high time consuming step**\n",
    "    normalized_range = _normalize_range(df, indicators_groups, align)\n",
    "\n",
    "    # predict over the range\n",
    "    cnn_prediction = np.zeros(len(normalized_range))\n",
    "    for i in range(0, len(normalized_range), gpu_batch_size):\n",
    "        cnn_prediction[i:i + gpu_batch_size] = _output(\n",
    "            cnn,\n",
    "            normalized_range[\n",
    "                i:i + gpu_batch_size],\n",
    "            gpu_id=gpu_id)[:, 0]\n",
    "\n",
    "    logger.debug(\"done processing range\")\n",
    "\n",
    "    # low pass filter\n",
    "    cnn_prediction = np.where(cnn_prediction >= 0.0,\n",
    "                              cnn_prediction, np.zeros(len(cnn_prediction)))\n",
    "\n",
    "    output = copy.deepcopy(df[-len(cnn_prediction):])\n",
    "    output[\"confidence\"] = cnn_prediction\n",
    "\n",
    "    return output[[\"confidence\"]]\n",
    "\n",
    "\n",
    "def list_suggestions(algo_id, gpu_id=None):\n",
    "    \"\"\"Find the most similar points locally.\n",
    "\n",
    "    Find the most similar points around the points selected in the dataset\n",
    "    of the algorithm refered by `algo_id`, using CNN. Call `calc_score()`\n",
    "    and select the best non contiguous results.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        algo_id         (int): algorithm id.\n",
    "        gpu_id          (int): GPU id, optional, default: `env['GPU_ID']`.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame indexed by date ordered by decreasing\n",
    "        confidence value. For example:\n",
    "\n",
    "        Date                | confidence\n",
    "        --------------------+-------------\n",
    "        2015-09-09 07:55:00 | 0.845748\n",
    "        2015-09-22 10:05:00 | 0.737587\n",
    "        2015-09-16 09:25:00 | 0.723020\n",
    "\n",
    "    Example:\n",
    "        `suggestions = list_suggestions(42)`\n",
    "    \"\"\"\n",
    "\n",
    "    if gpu_id is None:\n",
    "        gpu_id = int(os.environ.get('GPU_ID', 0))\n",
    "\n",
    "    # get scores\n",
    "    output = calc_score(algo_id, gpu_id=gpu_id)\n",
    "    scores = output[\"confidence\"].values\n",
    "\n",
    "    # select local maxima\n",
    "    maxima = np.where(np.r_[True, scores[1:] > scores[:-1]]\n",
    "                      & np.r_[scores[:-1] > scores[1:], True])[0]\n",
    "\n",
    "    # sort by score\n",
    "    best_scores = np.array(sorted(\n",
    "        zip(maxima, [scores[i] for i in maxima]), key=lambda k: k[1],\n",
    "        reverse=True))\n",
    "\n",
    "    return output.iloc[best_scores[:, 0].astype(np.int32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.94 s, sys: 720 ms, total: 6.66 s\n",
      "Wall time: 52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "global _samples\n",
    "out = calc_score(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _prepare_samples_CNN(postive_data, negative_samples, indicators_groups,\n",
    "                         augment_factor=1, asymetric_augment=1):\n",
    "    \"\"\"Reshape samples to be fed into the CNN.\"\"\"\n",
    "\n",
    "    # selected_data.shape[0]*2 #10000\n",
    "    size_dataset = postive_data.shape[0] * augment_factor * 2\n",
    "    len_samples = postive_data.shape[1]\n",
    "\n",
    "    if augment_factor > 1:\n",
    "        positive_samples = np.repeat(postive_data, augment_factor,\n",
    "                                     axis=0)\n",
    "    else:\n",
    "        positive_samples = postive_data\n",
    "\n",
    "    # Reshape data for convenient use with CNN\n",
    "    data = np.concatenate(\n",
    "        (positive_samples, negative_samples)).astype(np.float32)\n",
    "    labels = np.array([[1]] * len(positive_samples) + [[-1]]\n",
    "                      * len(negative_samples)).astype(np.int32)\n",
    "    data = np.swapaxes(data, 1, 2)\n",
    "    data = np.reshape(data, data.shape + (1,))\n",
    "\n",
    "    permut = np.random.permutation(len(data))\n",
    "    data = data[permut]\n",
    "    labels = labels[permut]\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mp, dataset, sc, bt_start, bt_end = _get_algo_data(1)\n",
    "\n",
    "_discard_grad_and_normalizers(mp)\n",
    "\n",
    "indicators = [str(i[\"id\"]) for i in mp[\"features\"]]\n",
    "indicators_groups = _get_indicators_group(mp)\n",
    "\n",
    "# negative inputs are not handled yet\n",
    "# _check_negative_inputs(dataset)\n",
    "#################\n",
    "global positive_ranges\n",
    "positive_ranges = [i for i in dataset if i['label']==[]]\n",
    "negative_ranges = [i for i in dataset if i['label']=='neg_icon']\n",
    "\n",
    "# ensure to load everything we will need\n",
    "ds_start = tm.T(dataset[0][\"start\"]).to_pydatetime()\n",
    "ds_end = tm.T(dataset[-1][\"end\"]).to_pydatetime()\n",
    "_df_start = min(ds_start, bt_start)\n",
    "_df_end = max(ds_end, bt_end)\n",
    "\n",
    "# get the DataFrame corresponding to dataset range plus margins\n",
    "# **high time consuming step**\n",
    "df = _load_df_range(mp, indicators, start=_df_start, end=_df_end)\n",
    "\n",
    "# get positive samples from the dataset\n",
    "x_and_y = zip(*[_select_normalize_from_df(sample,\n",
    "                                          df,\n",
    "                                          align,\n",
    "                                          indicators_groups,\n",
    "                                          justify_right=True\n",
    "                                          ) for sample in positive_ranges])\n",
    "selected_data, labels = np.array(x_and_y[0]), np.array(x_and_y[1])\n",
    "\n",
    "# get random negative samples\n",
    "# _align = min(100000, len(df))\n",
    "# tmp, _ = _select_normalize_from_df({u'end': u'2005/01/01 17:45',\n",
    "#                                     u'start': u'2018/12/31 15:00'},\n",
    "#                                    df, _align, indicators_groups,\n",
    "#                                    justify_right=False)\n",
    "global _samples\n",
    "if len(positive_ranges) > len(negative_ranges):\n",
    "    tmp = []\n",
    "    zer = df.index[0].strftime(\"%Y/%m/%d %H:%M\") #%z\n",
    "    add_n_samples = len(positive_ranges)- len(negative_ranges)\n",
    "    _samples = df[align:].sample(n=add_n_samples).index.sort_values()\n",
    "    for i in _samples: # .strftime(\"%Y/%m/%d %H:%M\"): #%Y-%m-%d %H-%M-%S\n",
    "        _start = i.strftime(\"%Y/%m/%d %H:%M\")\n",
    "        _end = (i - datetime.timedelta(align)).strftime(\"%Y/%m/%d %H:%M\")\n",
    "        tmp.append({u'end': _start, u'label':'neg_icon', u'start': _end})\n",
    "\n",
    "    negative_ranges += tmp\n",
    "\n",
    "\n",
    "\n",
    "neg = zip(*[_select_normalize_from_df(sample,\n",
    "                                          df,\n",
    "                                          align,\n",
    "                                          indicators_groups,\n",
    "                                          justify_right=True\n",
    "                                          ) for sample in negative_ranges])\n",
    "data_indicators = np.array(neg[0])\n",
    "\n",
    "# prepare the data to feed into the neural network\n",
    "samples, labels = _prepare_samples_CNN(selected_data, data_indicators,\n",
    "                                       indicators_groups,\n",
    "                                       augment_factor=1,\n",
    "                                       asymetric_augment=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
