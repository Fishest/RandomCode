{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is a sample code for DCGAN, I wil try it out for future work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "N_in = 1\n",
    "N_hidden = 64\n",
    "N_data = 100\n",
    "\n",
    "def infererence(input, h1_w, h1_b, out_w, out_b):\n",
    "    h1_out = tf.nn.relu(tf.matmul(input, h1_w) + h1_b)\n",
    "    output = tf.sigmoid(tf.matmul(h1_out, out_w) + out_b)\n",
    "    \n",
    "def trainable_inference(input):\n",
    "    h1_w = tf.get_variable(\n",
    "        \"d_h1_w\"\n",
    "        [N_in, N_hidden],\n",
    "        initializer = tf.random_normal_initializer(0, 0.1)\n",
    "    )\n",
    "    h1_b = tf.get_variable(\n",
    "        \"d_h1_b\",\n",
    "        [N_hidden],\n",
    "        initializer = tf.constant_initializer(0.1)\n",
    "    )\n",
    "    out_w = tf.get_variable(\n",
    "        \"d_out_w\",\n",
    "        [N_hidden, 1],\n",
    "        initializer = tf.random_normal_initializer(0, 0.1),\n",
    "    )\n",
    "    out_b = tf.get_variable(\n",
    "        \"d_out_b\",\n",
    "        [1],\n",
    "        initializer = tf.constant_initializer(0.1)\n",
    "    )\n",
    "    return inference(input, h1_w, h1_b, out_w, out_b)\n",
    "\n",
    "def get_train_params():\n",
    "    h1_w = tf.get_variable(\"d_h1_w\", [N_in, N_hidden])\n",
    "    h1_b = tf.ger_variable(\"d_h1_bias\", [N_hidden])\n",
    "    out_w = tf.get_variable(\"d_out_w\", [N_hidden, 1])\n",
    "    out_b = tf.get_variable(\"d_out_b\", [1])\n",
    "    return [h1_w, h1_b, out_w, out_b]\n",
    "\n",
    "def loss(output_from_given_data, output_from_noise):\n",
    "    with tf.name_scope(\"d_loss\") as scope:\n",
    "        loss_1 = tf.reduce_sum(tf.log(output_from_given_dasta))\n",
    "        loss_2 = tf.get_reduce_sum(tf.log(1 - output_from_noise))\n",
    "    return [loss_1, loss_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ops.py\n",
    "\n",
    "import math\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "# from utils import *\n",
    "\n",
    "class batch_norm(object):\n",
    "    \"\"\"Code modification of http://stackoverflow.com/a/33950177\"\"\"\n",
    "    def __init__(self, epsilon=1e-5, momentum = 0.9, name=\"batch_norm\"):\n",
    "        with tf.variable_scope(name):\n",
    "            self.epsilon = epsilon\n",
    "            self.momentum = momentum\n",
    "\n",
    "            self.ema = tf.train.ExponentialMovingAverage(decay=self.momentum)\n",
    "            self.name = name\n",
    "\n",
    "    def __call__(self, x, train=True):\n",
    "        shape = x.get_shape().as_list()\n",
    "\n",
    "        if train:\n",
    "            with tf.variable_scope(self.name) as scope:\n",
    "                self.beta = tf.get_variable(\"beta\", [shape[-1]],\n",
    "                                    initializer=tf.constant_initializer(0.))\n",
    "                self.gamma = tf.get_variable(\"gamma\", [shape[-1]],\n",
    "                                    initializer=tf.random_normal_initializer(1., 0.02))\n",
    "\n",
    "                batch_mean, batch_var = tf.nn.moments(x, [0, 1, 2], name='moments')\n",
    "                ema_apply_op = self.ema.apply([batch_mean, batch_var])\n",
    "                self.ema_mean, self.ema_var = self.ema.average(batch_mean), self.ema.average(batch_var)\n",
    "\n",
    "                with tf.control_dependencies([ema_apply_op]):\n",
    "                    mean, var = tf.identity(batch_mean), tf.identity(batch_var)\n",
    "        else:\n",
    "            mean, var = self.ema_mean, self.ema_var\n",
    "\n",
    "        normed = tf.nn.batch_norm_with_global_normalization(\n",
    "                x, mean, var, self.beta, self.gamma, self.epsilon, scale_after_normalization=True)\n",
    "\n",
    "        return normed\n",
    "\n",
    "def binary_cross_entropy(preds, targets, name=None):\n",
    "    \"\"\"Computes binary cross entropy given `preds`.\n",
    "    For brevity, let `x = `, `z = targets`.  The logistic loss is\n",
    "        loss(x, z) = - sum_i (x[i] * log(z[i]) + (1 - x[i]) * log(1 - z[i]))\n",
    "    Args:\n",
    "        preds: A `Tensor` of type `float32` or `float64`.\n",
    "        targets: A `Tensor` of the same type and shape as `preds`.\n",
    "    \"\"\"\n",
    "    eps = 1e-12\n",
    "    with ops.op_scope([preds, targets], name, \"bce_loss\") as name:\n",
    "        preds = ops.convert_to_tensor(preds, name=\"preds\")\n",
    "        targets = ops.convert_to_tensor(targets, name=\"targets\")\n",
    "        return tf.reduce_mean(-(targets * tf.log(preds + eps) +\n",
    "                              (1. - targets) * tf.log(1. - preds + eps)))\n",
    "\n",
    "def conv_cond_concat(x, y):\n",
    "    \"\"\"Concatenate conditioning vector on feature map axis.\"\"\"\n",
    "    x_shapes = x.get_shape()\n",
    "    y_shapes = y.get_shape()\n",
    "    return tf.concat(3, [x, y*tf.ones([x_shapes[0], x_shapes[1], x_shapes[2], y_shapes[3]])])\n",
    "\n",
    "def conv2d(input_, output_dim, \n",
    "           k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02,\n",
    "           name=\"conv2d\"):\n",
    "    with tf.variable_scope(name):\n",
    "        w = tf.get_variable('w', [k_h, k_w, input_.get_shape()[-1], output_dim],\n",
    "                            initializer=tf.truncated_normal_initializer(stddev=stddev))\n",
    "        conv = tf.nn.conv2d(input_, w, strides=[1, d_h, d_w, 1], padding='SAME')\n",
    "\n",
    "        biases = tf.get_variable('biases', [output_dim], initializer=tf.constant_initializer(0.0))\n",
    "        conv = tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape())\n",
    "\n",
    "        return conv\n",
    "\n",
    "def deconv2d(input_, output_shape,\n",
    "             k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02,\n",
    "             name=\"deconv2d\", with_w=False):\n",
    "    with tf.variable_scope(name):\n",
    "        # filter : [height, width, output_channels, in_channels]\n",
    "        w = tf.get_variable('w', [k_h, k_h, output_shape[-1], input_.get_shape()[-1]],\n",
    "                            initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "        \n",
    "        try:\n",
    "            deconv = tf.nn.conv2d_transpose(input_, w, output_shape=output_shape,\n",
    "                                strides=[1, d_h, d_w, 1])\n",
    "\n",
    "        # Support for verisons of TensorFlow before 0.7.0\n",
    "        except AttributeError:\n",
    "            deconv = tf.nn.deconv2d(input_, w, output_shape=output_shape,\n",
    "                                strides=[1, d_h, d_w, 1])\n",
    "\n",
    "        biases = tf.get_variable('biases', [output_shape[-1]], initializer=tf.constant_initializer(0.0))\n",
    "        deconv = tf.reshape(tf.nn.bias_add(deconv, biases), deconv.get_shape())\n",
    "\n",
    "        if with_w:\n",
    "            return deconv, w, biases\n",
    "        else:\n",
    "            return deconv\n",
    "\n",
    "def lrelu(x, leak=0.2, name=\"lrelu\"):\n",
    "    with tf.variable_scope(name):\n",
    "        f1 = 0.5 * (1 + leak)\n",
    "        f2 = 0.5 * (1 - leak)\n",
    "        return f1 * x + f2 * abs(x)\n",
    "\n",
    "def linear(input_, output_size, scope=None, stddev=0.02, bias_start=0.0, with_w=False):\n",
    "    shape = input_.get_shape().as_list()\n",
    "\n",
    "    with tf.variable_scope(scope or \"Linear\"):\n",
    "        matrix = tf.get_variable(\"Matrix\", [shape[1], output_size], tf.float32,\n",
    "                                 tf.random_normal_initializer(stddev=stddev))\n",
    "        bias = tf.get_variable(\"bias\", [output_size],\n",
    "            initializer=tf.constant_initializer(bias_start))\n",
    "        if with_w:\n",
    "            return tf.matmul(input_, matrix) + bias, matrix, bias\n",
    "        else:\n",
    "            return tf.matmul(input_, matrix) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from ops import *\n",
    "\n",
    "import os\n",
    "import time\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "\n",
    "# from ops import *\n",
    "# from utils import *\n",
    "\n",
    "class DCGAN(object):\n",
    "    def __init__(self, sess, image_size=108, is_crop=True,\n",
    "                 batch_size=64, sample_size = 64, image_shape=[64, 64, 3],\n",
    "                 y_dim=None, z_dim=100, gf_dim=64, df_dim=64,\n",
    "                 gfc_dim=1024, dfc_dim=1024, c_dim=3, dataset_name='default',\n",
    "                 checkpoint_dir=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: TensorFlow session\n",
    "            batch_size: The size of batch. Should be specified before training.\n",
    "            y_dim: (optional) Dimension of dim for y. [None]\n",
    "            z_dim: (optional) Dimension of dim for Z. [100]\n",
    "            gf_dim: (optional) Dimension of gen filters in first conv layer. [64]\n",
    "            df_dim: (optional) Dimension of discrim filters in first conv layer. [64]\n",
    "            gfc_dim: (optional) Dimension of gen untis for for fully connected layer. [1024]\n",
    "            dfc_dim: (optional) Dimension of discrim units for fully connected layer. [1024]\n",
    "            c_dim: (optional) Dimension of image color. [3]\n",
    "        \"\"\"\n",
    "        self.sess = sess\n",
    "        self.is_crop = is_crop\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.sample_size = sample_size\n",
    "        self.image_shape = image_shape\n",
    "\n",
    "        self.y_dim = y_dim\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        self.gf_dim = gf_dim\n",
    "        self.df_dim = df_dim\n",
    "\n",
    "        self.gfc_dim = gfc_dim\n",
    "        self.dfc_dim = dfc_dim\n",
    "\n",
    "        self.c_dim = 3\n",
    "\n",
    "        # batch normalization : deals with poor initialization helps gradient flow\n",
    "        self.d_bn1 = batch_norm(name='d_bn1')\n",
    "        self.d_bn2 = batch_norm(name='d_bn2')\n",
    "\n",
    "        if not self.y_dim:\n",
    "            self.d_bn3 = batch_norm(name='d_bn3')\n",
    "\n",
    "        self.g_bn0 = batch_norm(name='g_bn0')\n",
    "        self.g_bn1 = batch_norm(name='g_bn1')\n",
    "        self.g_bn2 = batch_norm(name='g_bn2')\n",
    "\n",
    "        if not self.y_dim:\n",
    "            self.g_bn3 = batch_norm(name='g_bn3')\n",
    "\n",
    "        self.dataset_name = dataset_name\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        if self.y_dim:\n",
    "            self.y= tf.placeholder(tf.float32, [None, self.y_dim], name='y')\n",
    "\n",
    "        self.images = tf.placeholder(tf.float32, [self.batch_size] + self.image_shape,\n",
    "                                    name='real_images')\n",
    "        self.sample_images= tf.placeholder(tf.float32, [self.sample_size] + self.image_shape,\n",
    "                                        name='sample_images')\n",
    "        self.z = tf.placeholder(tf.float32, [None, self.z_dim],\n",
    "                                name='z')\n",
    "\n",
    "        self.z_sum = tf.histogram_summary(\"z\", self.z)\n",
    "\n",
    "        self.G = self.generator(self.z)\n",
    "        self.D, self.D_logits = self.discriminator(self.images)\n",
    "\n",
    "        self.sampler = self.sampler(self.z)\n",
    "        self.D_, self.D_logits_ = self.discriminator(self.G, reuse=True)\n",
    "\n",
    "        self.d_sum = tf.histogram_summary(\"d\", self.D)\n",
    "        self.d__sum = tf.histogram_summary(\"d_\", self.D_)\n",
    "        self.G_sum = tf.image_summary(\"G\", self.G)\n",
    "\n",
    "        self.d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(self.D_logits, tf.ones_like(self.D)))\n",
    "        self.d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(self.D_logits_, tf.zeros_like(self.D_)))\n",
    "        self.g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(self.D_logits_, tf.ones_like(self.D_)))\n",
    "\n",
    "        self.d_loss_real_sum = tf.scalar_summary(\"d_loss_real\", self.d_loss_real)\n",
    "        self.d_loss_fake_sum = tf.scalar_summary(\"d_loss_fake\", self.d_loss_fake)\n",
    "                                                    \n",
    "        self.d_loss = self.d_loss_real + self.d_loss_fake\n",
    "\n",
    "        self.g_loss_sum = tf.scalar_summary(\"g_loss\", self.g_loss)\n",
    "        self.d_loss_sum = tf.scalar_summary(\"d_loss\", self.d_loss)\n",
    "\n",
    "        t_vars = tf.trainable_variables()\n",
    "\n",
    "        self.d_vars = [var for var in t_vars if 'd_' in var.name]\n",
    "        self.g_vars = [var for var in t_vars if 'g_' in var.name]\n",
    "\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "    def train(self, config):\n",
    "        \"\"\"Train DCGAN\"\"\"\n",
    "        data = glob(os.path.join(\"./data\", config.dataset, \"*.jpg\"))\n",
    "        #np.random.shuffle(data)\n",
    "\n",
    "        d_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \\\n",
    "                          .minimize(self.d_loss, var_list=self.d_vars)\n",
    "        g_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \\\n",
    "                          .minimize(self.g_loss, var_list=self.g_vars)\n",
    "        tf.initialize_all_variables().run()\n",
    "\n",
    "        self.saver = tf.train.Saver()\n",
    "        self.g_sum = tf.merge_summary([self.z_sum, self.d__sum, \n",
    "            self.G_sum, self.d_loss_fake_sum, self.g_loss_sum])\n",
    "        self.d_sum = tf.merge_summary([self.z_sum, self.d_sum, self.d_loss_real_sum, self.d_loss_sum])\n",
    "        self.writer = tf.train.SummaryWriter(\"./logs\", self.sess.graph_def)\n",
    "\n",
    "        sample_z = np.random.uniform(-1, 1, size=(self.sample_size , self.z_dim))\n",
    "        sample_files = data[0:self.sample_size]\n",
    "        sample = [get_image(sample_file, self.image_size, is_crop=self.is_crop) for sample_file in sample_files]\n",
    "        sample_images = np.array(sample).astype(np.float32)\n",
    "\n",
    "        counter = 1\n",
    "        start_time = time.time()\n",
    "\n",
    "        if self.load(self.checkpoint_dir):\n",
    "            print(\" [*] Load SUCCESS\")\n",
    "        else:\n",
    "            print(\" [!] Load failed...\")\n",
    "\n",
    "        for epoch in xrange(config.epoch):\n",
    "            data = glob(os.path.join(\"./data\", config.dataset, \"*.jpg\"))\n",
    "            batch_idxs = min(len(data), config.train_size)/config.batch_size\n",
    "\n",
    "            for idx in xrange(0, batch_idxs):\n",
    "                batch_files = data[idx*config.batch_size:(idx+1)*config.batch_size]\n",
    "                batch = [get_image(batch_file, self.image_size, is_crop=self.is_crop) for batch_file in batch_files]\n",
    "                batch_images = np.array(batch).astype(np.float32)\n",
    "\n",
    "                batch_z = np.random.uniform(-1, 1, [config.batch_size, self.z_dim]) \\\n",
    "                            .astype(np.float32)\n",
    "\n",
    "                # Update D network\n",
    "                _, summary_str = self.sess.run([d_optim, self.d_sum],\n",
    "                    feed_dict={ self.images: batch_images, self.z: batch_z })\n",
    "                self.writer.add_summary(summary_str, counter)\n",
    "\n",
    "                # Update G network\n",
    "                _, summary_str = self.sess.run([g_optim, self.g_sum],\n",
    "                    feed_dict={ self.z: batch_z })\n",
    "                self.writer.add_summary(summary_str, counter)\n",
    "\n",
    "                # Run g_optim twice to make sure that d_loss does not go to zero (different from paper)\n",
    "                _, summary_str = self.sess.run([g_optim, self.g_sum],\n",
    "                    feed_dict={ self.z: batch_z })\n",
    "                self.writer.add_summary(summary_str, counter)\n",
    "\n",
    "                errD_fake = self.d_loss_fake.eval({self.z: batch_z})\n",
    "                errD_real = self.d_loss_real.eval({self.images: batch_images})\n",
    "                errG = self.g_loss.eval({self.z: batch_z})\n",
    "\n",
    "                counter += 1\n",
    "                print(\"Epoch: [%2d] [%4d/%4d] time: %4.4f, d_loss: %.8f, g_loss: %.8f\" \\\n",
    "                    % (epoch, idx, batch_idxs,\n",
    "                        time.time() - start_time, errD_fake+errD_real, errG))\n",
    "\n",
    "                if np.mod(counter, 100) == 1:\n",
    "                    samples, d_loss, g_loss = self.sess.run(\n",
    "                        [self.sampler, self.d_loss, self.g_loss],\n",
    "                        feed_dict={self.z: sample_z, self.images: sample_images}\n",
    "                    )\n",
    "                    save_images(samples, [8, 8],\n",
    "                                './samples/train_%s_%s.png' % (epoch, idx))\n",
    "                    print(\"[Sample] d_loss: %.8f, g_loss: %.8f\" % (d_loss, g_loss))\n",
    "\n",
    "                if np.mod(counter, 500) == 2:\n",
    "                    self.save(config.checkpoint_dir, counter)\n",
    "\n",
    "    def discriminator(self, image, reuse=False, y=None):\n",
    "        if reuse:\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "        if not self.y_dim:\n",
    "            h0 = lrelu(conv2d(image, self.df_dim, name='d_h0_conv'))\n",
    "            h1 = lrelu(self.d_bn1(conv2d(h0, self.df_dim*2, name='d_h1_conv')))\n",
    "            h2 = lrelu(self.d_bn2(conv2d(h1, self.df_dim*4, name='d_h2_conv')))\n",
    "            h3 = lrelu(self.d_bn3(conv2d(h2, self.df_dim*8, name='d_h3_conv')))\n",
    "            h4 = linear(tf.reshape(h3, [self.batch_size, -1]), 1, 'd_h3_lin')\n",
    "\n",
    "            return tf.nn.sigmoid(h4), h4\n",
    "        else:\n",
    "            yb = tf.reshape(y, [None, 1, 1, self.y_dim])\n",
    "            x = conv_cond_concat(image, yb)\n",
    "\n",
    "            h0 = lrelu(spatial_conv(x, self.c_dim + self.y_dim))\n",
    "            h0 = conv_cond_concat(h0, yb)\n",
    "\n",
    "            h1 = lrelu(self.d_bn1(conv2d(h0, self.df_dim + self.y_dim)))\n",
    "            h1 = tf.reshape(h1, [h1.get_shape()[0], -1])\n",
    "            h1 = tf.concat(1, [h1, y])\n",
    "\n",
    "            h2 = lrelu(self.d_bn2(linear(h1, self.dfc_dim, 'd_h2_lin')))\n",
    "            h2 = tf.concat(1, [h2, y])\n",
    "\n",
    "            return tf.nn.sigmoid(linear(h2, 1, 'd_h3_lin'))\n",
    "\n",
    "    def generator(self, z, y=None):\n",
    "        if not self.y_dim:\n",
    "            # project `z` and reshape\n",
    "            self.z_, self.h0_w, self.h0_b = linear(z, self.gf_dim*8*4*4, 'g_h0_lin', with_w=True)\n",
    "\n",
    "            self.h0 = tf.reshape(self.z_, [-1, 4, 4, self.gf_dim * 8])\n",
    "            h0 = tf.nn.relu(self.g_bn0(self.h0))\n",
    "\n",
    "            self.h1, self.h1_w, self.h1_b = deconv2d(h0, \n",
    "                [self.batch_size, 8, 8, self.gf_dim*4], name='g_h1', with_w=True)\n",
    "            h1 = tf.nn.relu(self.g_bn1(self.h1))\n",
    "\n",
    "            h2, self.h2_w, self.h2_b = deconv2d(h1,\n",
    "                [self.batch_size, 16, 16, self.gf_dim*2], name='g_h2', with_w=True)\n",
    "            h2 = tf.nn.relu(self.g_bn2(h2))\n",
    "\n",
    "            h3, self.h3_w, self.h3_b = deconv2d(h2,\n",
    "                [self.batch_size, 32, 32, self.gf_dim*1], name='g_h3', with_w=True)\n",
    "            h3 = tf.nn.relu(self.g_bn3(h3))\n",
    "\n",
    "            h4, self.h4_w, self.h4_b = deconv2d(h3,\n",
    "                [self.batch_size, 64, 64, 3], name='g_h4', with_w=True)\n",
    "\n",
    "            return tf.nn.tanh(h4)\n",
    "        else:\n",
    "            yb = tf.reshape(y, [None, 1, 1, self.y_dim])\n",
    "            z = tf.concat(1, [z, y])\n",
    "\n",
    "            h0 = tf.nn.relu(self.g_bn0(linear(z, self.gfc_dim, 'g_h0_lin')))\n",
    "            h0 = tf.concat(1, [h0, y])\n",
    "\n",
    "            h1 = tf.nn.relu(self.g_bn1(linear(z, self.gf_dim*2*7*7, 'g_h1_lin')))\n",
    "            h1 = tf.reshape(h1, [None, 7, 7, self.gf_dim * 2])\n",
    "            h1 = conv_cond_concat(h1, yb)\n",
    "\n",
    "            h2 = tf.nn.relu(self.g_bn2(deconv2d(h1, self.gf_dim, name='g_h2')))\n",
    "            h2 = conv_cond_concat(h2, yb)\n",
    "\n",
    "            return tf.nn.sigmoid(deconv2d(h2, self.c_dim, name='g_h3'))\n",
    "\n",
    "    def sampler(self, z, y=None):\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "        if not self.y_dim:\n",
    "            # project `z` and reshape\n",
    "            h0 = tf.reshape(linear(z, self.gf_dim*8*4*4, 'g_h0_lin'),\n",
    "                            [-1, 4, 4, self.gf_dim * 8])\n",
    "            h0 = tf.nn.relu(self.g_bn0(h0, train=False))\n",
    "\n",
    "            h1 = deconv2d(h0, [self.batch_size, 8, 8, self.gf_dim*4], name='g_h1')\n",
    "            h1 = tf.nn.relu(self.g_bn1(h1, train=False))\n",
    "\n",
    "            h2 = deconv2d(h1, [self.batch_size, 16, 16, self.gf_dim*2], name='g_h2')\n",
    "            h2 = tf.nn.relu(self.g_bn2(h2, train=False))\n",
    "\n",
    "            h3 = deconv2d(h2, [self.batch_size, 32, 32, self.gf_dim*1], name='g_h3')\n",
    "            h3 = tf.nn.relu(self.g_bn3(h3, train=False))\n",
    "\n",
    "            h4 = deconv2d(h3, [self.batch_size, 64, 64, 3], name='g_h4')\n",
    "\n",
    "            return tf.nn.tanh(h4)\n",
    "        else:\n",
    "            yb = tf.reshape(y, [None, 1, 1, self.y_dim])\n",
    "            z = tf.concat(1, [z, y])\n",
    "\n",
    "            h0 = tf.nn.relu(self.bn0(linear(z, self.gfc_dim, 'g_h0_lin')))\n",
    "            h0 = tf.concat(1, [h0, y])\n",
    "\n",
    "            h1 = tf.nn.relu(self.g_bn1(linear(z, self.gf_dim*2*7*7, 'g_h1_lin')))\n",
    "            h1 = tf.reshape(h1, [None, 7, 7, self.gf_dim * 2])\n",
    "            h1 = conv_cond_concat(h1, yb)\n",
    "\n",
    "            h2 = tf.nn.relu(self.bn2(deconv2d(h1, self.gf_dim, name='g_h2')))\n",
    "            h2 = conv_cond_concat(h2, yb)\n",
    "\n",
    "            return tf.nn.sigmoid(deconv2d(h2, self.c_dim, name='g_h3'))\n",
    "\n",
    "    def save(self, checkpoint_dir, step):\n",
    "        model_name = \"DCGAN.model\"\n",
    "        model_dir = \"%s_%s\" % (self.dataset_name, self.batch_size)\n",
    "        checkpoint_dir = os.path.join(checkpoint_dir, model_dir)\n",
    "\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "\n",
    "        self.saver.save(self.sess,\n",
    "                        os.path.join(checkpoint_dir, model_name),\n",
    "                        global_step=step)\n",
    "\n",
    "    def load(self, checkpoint_dir):\n",
    "        print(\" [*] Reading checkpoints...\")\n",
    "\n",
    "        model_dir = \"%s_%s\" % (self.dataset_name, self.batch_size)\n",
    "        checkpoint_dir = os.path.join(checkpoint_dir, model_dir)\n",
    "\n",
    "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "            self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
