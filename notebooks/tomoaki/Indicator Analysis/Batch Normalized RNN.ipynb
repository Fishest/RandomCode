{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make learning faster, we'll introduce batch normalization for RNN.\n",
    "\n",
    "I refered to the following paper:\n",
    "https://arxiv.org/pdf/1603.09025.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: matplotlib in /usr/local/lib/python2.7/dist-packages\n",
      "Requirement already up-to-date: numpy>=1.6 in /usr/local/lib/python2.7/dist-packages (from matplotlib)\n",
      "Requirement already up-to-date: python-dateutil in /usr/local/lib/python2.7/dist-packages (from matplotlib)\n",
      "Requirement already up-to-date: pytz in /usr/local/lib/python2.7/dist-packages (from matplotlib)\n",
      "Requirement already up-to-date: cycler in /usr/local/lib/python2.7/dist-packages (from matplotlib)\n",
      "Requirement already up-to-date: pyparsing!=2.0.4,>=1.5.6 in /usr/local/lib/python2.7/dist-packages (from matplotlib)\n",
      "Cleaning up...\n",
      "Requirement already up-to-date: quandl in /usr/local/lib/python2.7/dist-packages\n",
      "Cleaning up...\n",
      "Requirement already up-to-date: yahoo-finance in /usr/local/lib/python2.7/dist-packages\n",
      "Requirement already up-to-date: pytz in /usr/local/lib/python2.7/dist-packages (from yahoo-finance)\n",
      "Requirement already up-to-date: simplejson in /usr/local/lib/python2.7/dist-packages (from yahoo-finance)\n",
      "Cleaning up...\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib --upgrade\n",
    "!pip install quandl --upgrade\n",
    "!pip install yahoo-finance --upgrade\n",
    "\n",
    "#generic\n",
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import quandl \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from yahoo_finance import Share\n",
    "\n",
    "\n",
    "\n",
    "def get_data_by_key(key, data):\n",
    "    data_it = iter(data)\n",
    "    return_data = []\n",
    "    flag = True\n",
    "    for d in data_it:\n",
    "        return_data.append(d[key])\n",
    "        \n",
    "    return np.array(return_data)\n",
    "\n",
    "def get_data_by_list(name_list, start_date, end_date, data_type=\"Open\"):\n",
    "    share_list = []\n",
    "    new_name_list = []\n",
    "    for name in name_list:\n",
    "        try:\n",
    "            share_list.append(Share(name))\n",
    "            new_name_list.append(name)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    stock_data_list = []\n",
    "    date = []\n",
    "    flag = True\n",
    "    N_data = 0\n",
    "    fail_name_list = []\n",
    "    ret_name_list = []\n",
    "    for idx, share in enumerate(share_list):\n",
    "        name = new_name_list[idx]\n",
    "        try:\n",
    "            hist_data = share.get_historical(start_date=start_date, end_date=end_date)\n",
    "            stock_data = map(float, get_data_by_key(key=data_type, data=hist_data))\n",
    "            n_data = len(stock_data)\n",
    "            if n_data == 0:\n",
    "                fail_name_list.append(name)\n",
    "            date.append(get_data_by_key(key='Date', data=hist_data))\n",
    "            stock_data_list.append(stock_data)\n",
    "            ret_name_list.append(name)\n",
    "        except:\n",
    "            pass\n",
    "    print (\"fail_name_list: \", fail_name_list)\n",
    "    return np.array(stock_data_list).T, date, ret_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270\n"
     ]
    }
   ],
   "source": [
    "energy = [\"PTR\", \"XOM\", \"CVX\", \"RDS-A\", \"BP\", \"TOT\", \"SLB\", \"KMI\", \"COP\", \"CEO\", \n",
    "          \"E\", \"STO\", \"OXY\", \"PBR\", \"EOG\", \"APC\", \"SU\", \"ENB\", \"HAL\", \"WMB\"]\n",
    "financial = [\"WFC\", \"JPM\", \"HSBC\", \"BAC\", \"C\", \"SAN\", \"MTU\", \"RY\", \"WBK\", \"TD\", \"GS\", \n",
    "             \"LYG\", \"AXP\", \"AIG\", \"MS\", \"ITUB\", \"BCS\", \"BBVA\"]\n",
    "healthcare = [\"NVS\", \"JNJ\", \"PFE\", \"MRK\", \"GILD\", \"SNY\", \"AMGN\", \"NVO\", \"GSK\", \"UNH\",\n",
    "             \"MDT\", \"BMY\", \"CELG\", \"BIIB\", \"AZN\", \"LLY\", \"ABT\", \"AGN\", \"VRX\", \"TEVA\",\n",
    "             \"TMO\", \"SHPG\", \"REGN\"]\n",
    "buisiness = [\"ACN\", \"LMT\", \"CNI\", \"FDX\", \"DAL\", \"CSX\", \"AAL\", \"CP\", \"NSC\", \"NOC\", \"LUV\"]\n",
    "telecom = [\"CHL\", \"VZ\", \"T\", \"VOD\", \"NTT\", \"AMX\", \"CHA\", \"BT\", \"CHU\", \"ORAN\", \"BCE\",\n",
    "           \"CHT\", \"SKM\", \"TI\", \"TU\", \"S\", \"TLK\", \"DUK\", \"NGG\", \"NEE\", \"D\", \"SO\", \"EXC\", \"KEP\",\n",
    "          \"AEP\", \"SRE\", \"PCG\", \"HNP\", \"PPL\", \"PEG\", \"EIX\", \"ETP\", \"ED\", \"ENI\", \"XEL\", \"ES\", \"FE\"]\n",
    "hardware = [\"AAPL\", \"ORCL\", \"IBM\", \"INTC\", \"CSCO\", \"TSM\", \"QCOM\", \"HPQ\", \"TXN\", \"EMC\",\n",
    "           \"CAJ\", \"ASML\", \"ERIC\", \"SNE\", \"AVGO\", \"MU\", \"GLW\", \"NXPI\", \"NOK\",\n",
    "           \"AMAT\", \"WDC\", \"WIT\", \"ADI\", \"STX\", \"APH\"]\n",
    "software = [\"MSFT\", \"GOOGL\", \"BIDU\", \"EBAY\", \"SAP\", \"CRM\", \"YHOO\", \"VMW\",\n",
    "            \"ADBE\", \"CTSH\", \"INFY\", \"INTU\", \"LNKD\", \"RHT\", \"NTES\", \"CHKP\", \n",
    "            \"CA\", \"ADSK\", \"AKAM\", \"NVDA\"]\n",
    "industrial = [\"GE\", \"V\", \"MA\", \"UTX\", \"MMM\", \"BA\", \"UNP\", \"UPS\", \"HON\", \"DHR\", \"CAT\", \n",
    "              \"ABB\", \"GD\", \"ADP\", \"EMR\", \"ITW\", \"ECL\", \"TEL\", \"PCAR\", \"WM\"]\n",
    "manufacturing = [\"TM\", \"HMC\", \"F\", \"GM\", \"JCI\", \"TSLA\", \"TTM\", \"CMI\", \"DLPH\", \"MGA\",\n",
    "                \"CMI\", \"DLPH\", \"MGA\", \"GPC\", \"BWA\", \"HOG\", \"ALV\", \"HAR\", \"LEA\", \"LKQ\", \n",
    "                 \"WBC\", \"GT\", \"FCAU\", \"NSANY\", \"NAV\"]\n",
    "consumer = [\"PG\", \"BUD\", \"KO\", \"PEP\", \"UL\", \"PM\", \"BTI\", \"MO\", \"ABEV\", \"DEO\", \"MDLZ\",\n",
    "            \"CL\", \"MON\", \"MCK\", \"KMB\", \"WHR\", \"DIS\", \"CMCSA\", \"FOXA\", \"TWX\", \"TWC\",\n",
    "           \"DISH\", \"CBS\", \"DISCA\", \"TV\", \"CHTR\", \"QVCA\", \"OMC\", \"NLSN\", \n",
    "           \"PSO\", \"SJR\", \"NFLX\"]\n",
    "diversified = [\"BRK-A\", \"BRK-B\", \"UTX\", \"BC\", \"RTN\", \"OLN\", \"ITT\", \"MSBHY\", \"KWHIY\", \n",
    "               \"ABB\", \"IEP\", \"GE\"]\n",
    "retailing = [\"WMT\", \"AMZN\", \"HD\", \"MCD\", \"NKE\", \"SBUX\", \"CVS\", \"WBA\", \"PCLN\", \"COST\", \n",
    "            \"TGT\", \"TJX\", \"LVS\", \"YUM\", \"CCL\", \"LUX\", \"DG\", \"M\", \"AZO\", \"ROST\", \"CMG\", \"GPS\", \n",
    "            \"DLTR\", \"RCL\", \"KSS\", \"HOT\", \"JWN\"]\n",
    "\n",
    "input_list = energy + financial + healthcare + buisiness + telecom\\\n",
    "+ hardware + software + industrial + manufacturing + consumer + diversified + retailing\n",
    "print (len(input_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started!!\n",
      "fail_name_list:  ['NEE']\n",
      "time for getting training_data: 1020.07868218\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "print (\"Started!!\")\n",
    "\n",
    "st = time.time()\n",
    "start_date=\"2014-04-01\"\n",
    "end_date=\"2016-04-01\"\n",
    "input_data, date, input_list = get_data_by_list(input_list, start_date=start_date, end_date=end_date) \n",
    "sp = quandl.get(\"YAHOO/INDEX_GSPC\", start_date=start_date, end_date=end_date)\n",
    "target_data = np.array(sp['Open'])\n",
    "elapsed = time.time() - st\n",
    "print (\"time for getting training_data:\", elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEE\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "index = input_list.index(\"NEE\")\n",
    "print (input_list[index])\n",
    "print (input_data[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_data_tilde = np.delete(input_data, index, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(269, 505)\n"
     ]
    }
   ],
   "source": [
    "length = len(input_data_tilde[0])\n",
    "for i in xrange(len(input_data_tilde)):\n",
    "    if length != len(input_data_tilde[i]):\n",
    "        print (i)\n",
    "        \n",
    "data_list = []\n",
    "for data in iter(input_data_tilde):\n",
    "    data_list.append(data)\n",
    "data = np.array(data_list)\n",
    "print (data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_data = data.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269\n"
     ]
    }
   ],
   "source": [
    "energy = [\"PTR\", \"XOM\", \"CVX\", \"RDS-A\", \"BP\", \"TOT\", \"SLB\", \"KMI\", \"COP\", \"CEO\", \n",
    "          \"E\", \"STO\", \"OXY\", \"PBR\", \"EOG\", \"APC\", \"SU\", \"ENB\", \"HAL\", \"WMB\"]\n",
    "financial = [\"WFC\", \"JPM\", \"HSBC\", \"BAC\", \"C\", \"SAN\", \"MTU\", \"RY\", \"WBK\", \"TD\", \"GS\", \n",
    "             \"LYG\", \"AXP\", \"AIG\", \"MS\", \"ITUB\", \"BCS\", \"BBVA\"]\n",
    "healthcare = [\"NVS\", \"JNJ\", \"PFE\", \"MRK\", \"GILD\", \"SNY\", \"AMGN\", \"NVO\", \"GSK\", \"UNH\",\n",
    "             \"MDT\", \"BMY\", \"CELG\", \"BIIB\", \"AZN\", \"LLY\", \"ABT\", \"AGN\", \"VRX\", \"TEVA\",\n",
    "             \"TMO\", \"SHPG\", \"REGN\"]\n",
    "buisiness = [\"ACN\", \"LMT\", \"CNI\", \"FDX\", \"DAL\", \"CSX\", \"AAL\", \"CP\", \"NSC\", \"NOC\", \"LUV\"]\n",
    "telecom = [\"CHL\", \"VZ\", \"T\", \"VOD\", \"NTT\", \"AMX\", \"CHA\", \"BT\", \"CHU\", \"ORAN\", \"BCE\",\n",
    "           \"CHT\", \"SKM\", \"TI\", \"TU\", \"S\", \"TLK\", \"DUK\", \"NGG\", \"D\", \"SO\", \"EXC\", \"KEP\",\n",
    "          \"AEP\", \"SRE\", \"PCG\", \"HNP\", \"PPL\", \"PEG\", \"EIX\", \"ETP\", \"ED\", \"ENI\", \"XEL\", \"ES\", \"FE\"]\n",
    "hardware = [\"AAPL\", \"ORCL\", \"IBM\", \"INTC\", \"CSCO\", \"TSM\", \"QCOM\", \"HPQ\", \"TXN\", \"EMC\",\n",
    "           \"CAJ\", \"ASML\", \"ERIC\", \"SNE\", \"AVGO\", \"MU\", \"GLW\", \"NXPI\", \"NOK\",\n",
    "           \"AMAT\", \"WDC\", \"WIT\", \"ADI\", \"STX\", \"APH\"]\n",
    "software = [\"MSFT\", \"GOOGL\", \"BIDU\", \"EBAY\", \"SAP\", \"CRM\", \"YHOO\", \"VMW\",\n",
    "            \"ADBE\", \"CTSH\", \"INFY\", \"INTU\", \"LNKD\", \"RHT\", \"NTES\", \"CHKP\", \n",
    "            \"CA\", \"ADSK\", \"AKAM\", \"NVDA\"]\n",
    "industrial = [\"GE\", \"V\", \"MA\", \"UTX\", \"MMM\", \"BA\", \"UNP\", \"UPS\", \"HON\", \"DHR\", \"CAT\", \n",
    "              \"ABB\", \"GD\", \"ADP\", \"EMR\", \"ITW\", \"ECL\", \"TEL\", \"PCAR\", \"WM\"]\n",
    "manufacturing = [\"TM\", \"HMC\", \"F\", \"GM\", \"JCI\", \"TSLA\", \"TTM\", \"CMI\", \"DLPH\", \"MGA\",\n",
    "                \"CMI\", \"DLPH\", \"MGA\", \"GPC\", \"BWA\", \"HOG\", \"ALV\", \"HAR\", \"LEA\", \"LKQ\", \n",
    "                 \"WBC\", \"GT\", \"FCAU\", \"NSANY\", \"NAV\"]\n",
    "consumer = [\"PG\", \"BUD\", \"KO\", \"PEP\", \"UL\", \"PM\", \"BTI\", \"MO\", \"ABEV\", \"DEO\", \"MDLZ\",\n",
    "            \"CL\", \"MON\", \"MCK\", \"KMB\", \"WHR\", \"DIS\", \"CMCSA\", \"FOXA\", \"TWX\", \"TWC\",\n",
    "           \"DISH\", \"CBS\", \"DISCA\", \"TV\", \"CHTR\", \"QVCA\", \"OMC\", \"NLSN\", \n",
    "           \"PSO\", \"SJR\", \"NFLX\"]\n",
    "diversified = [\"BRK-A\", \"BRK-B\", \"UTX\", \"BC\", \"RTN\", \"OLN\", \"ITT\", \"MSBHY\", \"KWHIY\", \n",
    "               \"ABB\", \"IEP\", \"GE\"]\n",
    "retailing = [\"WMT\", \"AMZN\", \"HD\", \"MCD\", \"NKE\", \"SBUX\", \"CVS\", \"WBA\", \"PCLN\", \"COST\", \n",
    "            \"TGT\", \"TJX\", \"LVS\", \"YUM\", \"CCL\", \"LUX\", \"DG\", \"M\", \"AZO\", \"ROST\", \"CMG\", \"GPS\", \n",
    "            \"DLTR\", \"RCL\", \"KSS\", \"HOT\", \"JWN\"]\n",
    "\n",
    "input_list = energy + financial + healthcare + buisiness + telecom\\\n",
    "+ hardware + software + industrial + manufacturing + consumer + diversified + retailing\n",
    "print (len(input_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(505, 1)\n"
     ]
    }
   ],
   "source": [
    "target_data = np.atleast_2d(target_data).T\n",
    "print (target_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_input = input_data[1:] / input_data[:-1] - 1.0\n",
    "df_target = target_data[1:] / target_data[:-1] -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BNLSTM(object):\n",
    "    \n",
    "    def __init__(self, layers, activation=tf.sigmoid, is_initialize=True):\n",
    "        \"\"\"Initialize Vanilla LSTM with Batch Normalization\n",
    "        \n",
    "        Args:\n",
    "            layers (List(int)): each element should be the number of components\n",
    "            is_initialized(bool): if this variable is True, glaph will be initialized\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.activation = activation\n",
    "        \n",
    "        if is_initialize is True:\n",
    "            sess =tf.InteractiveSession()\n",
    "            tf.reset_default_graph()\n",
    "            sess.close()  \n",
    "        \n",
    "    def weight_variable(self, shape):\n",
    "        initial = tf.truncated_normal(shape, stddev=1.0)\n",
    "        return tf.Variable(initial, name=\"weight\")\n",
    "        \n",
    "    def bias_variable(self, shape):\n",
    "        initial = tf.constant(0.0, shape=shape)\n",
    "        return tf.Variable(initial, name=\"bias\")\n",
    "        \n",
    "    def batch_normalization(self, input, shape):\n",
    "        # input should be hidden_dim\n",
    "        eps = 1e-5\n",
    "        gamma = tf.Variable(tf.constant(1.0, shape=shape))\n",
    "        beta = tf.Variable(tf.constant(0.0, shape=shape))\n",
    "        mean, variance = tf.nn.moments(input, [0])\n",
    "        return gamma * (input - mean) / tf.sqrt(variance + eps) + beta\n",
    "    \n",
    "    def build_model(self, n_split):\n",
    "        # build model for trainig\n",
    "        self._input = tf.placeholder(tf.float32, [None, self.layers[0]])\n",
    "        self._target = tf.placeholder(tf.float32, [None, self.layers[-1]])\n",
    "        \n",
    "        # when learning, we will split data which has shape (T, dim)\n",
    "        # into (n_batch, dim) * T / n_batch\n",
    "        input_tilde = tf.unpack(tf.split(0, n_split, self._input))\n",
    "        target_tilde = tf.unpack(tf.split(0, n_split, self._target))\n",
    "        \n",
    "        # the name of variable scope will be layer0, layer1, ...\n",
    "        x = tf.concat(0, input_tilde)\n",
    "        for i_layer in xrange(len(self.layers) - 1):\n",
    "            with tf.variable_scope(\"layer%d\" % i_layer):\n",
    "                # build LSTM graph for each layer\n",
    "                self._cell = tf.nn.rnn_cell.BasicLSTMCell(self.layers[i_layer + 1])\n",
    "                self._initial_state = self._cell.zero_state(1, tf.float32)\n",
    "                cell_outputs, state = tf.nn.rnn(cell=self._cell, \n",
    "                                                                      inputs=input_tilde, \n",
    "                                                                      initial_state=self._initial_state)\n",
    "                self._final_state = state\n",
    "                n_in = self.layers[i_layer]\n",
    "                n_out = self.layers[i_layer + 1]\n",
    "                W = self.weight_variable([n_in, n_out])\n",
    "                b = self.bias_variable([n_out])\n",
    "                z = tf.matmul(x, W) + b\n",
    "            # we will not apply activation function for the last layer\n",
    "            if i_layer == len(self.layers) - 2:\n",
    "                self._output = z\n",
    "            else:\n",
    "                shape = [self.layers[i_layer + 1]]\n",
    "                normalized_z = self.batch_normalization(z, shape)\n",
    "                x = self.activation(normalized_z)\n",
    "        \n",
    "        # training graph\n",
    "        # self._learning_rate = tf.placeholder(tf.float32, shape=[], name=\"learning rate\")\n",
    "        self.loss = tf.reduce_mean(tf.square(self._output - self._target))\n",
    "        # optimizer = tf.train.AdamOptimizer(self._learning_rate)\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        self.train = optimizer.minimize(self.loss) \n",
    "    \n",
    "    def training(self, input_data, target_data, n_batch=20, n_epochs=100,\n",
    "                         learning_rate=1e-8, decay_rate=0.5, decay_freq=100):\n",
    "        n_data = len(input_data)\n",
    "        print_freq = int(n_epochs / 10)\n",
    "        \n",
    "        # build training graph\n",
    "        n_split = int(n_data / n_batch)\n",
    "        self.build_model(n_split)\n",
    "        \n",
    "        # we'll use only recent n_batch * n_split data \n",
    "        # because of garph's architechture\n",
    "        input_data = input_data[n_data - n_batch * n_split:]\n",
    "        target_data = input_data[n_data - n_batch * n_split:]\n",
    "        \n",
    "        # Add ops to save and restore all the variables.\n",
    "        saver = tf.train.Saver()\n",
    "        init_op = tf.initialize_all_variables()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_op)\n",
    "            n_iter = int(n_data / n_batch)\n",
    "            batch_index = np.arange(n_iter - 1)   # index of data for shuffling\n",
    "            for epoch in xrange(n_epochs):\n",
    "                \"\"\"\n",
    "                sess.run(self.train, \n",
    "                                feed_dict={self._input: input_data,\n",
    "                                                     self._target: target_data,\n",
    "                                                     self._learning_rate: learning_rate})\n",
    "                \"\"\"\n",
    "                sess.run(self.train, \n",
    "                                feed_dict={self._input: input_data,\n",
    "                                                     self._target: target_data})\n",
    "                if (epoch + 1) % decay_freq == 0:\n",
    "                    learning_rate *= decay_rate\n",
    "                    print (\"change learning rate\", learning_rate)\n",
    "                if epoch % print_freq == 0:\n",
    "                    print (\"loss:\", \n",
    "                               self.loss.eval(session=sess, \n",
    "                                                      feed_dict={self._input: input_data,\n",
    "                                                                           self._target: target_data}))     \n",
    "            save_path = saver.save(sess, \"/jupyter/tomoaki/model.ckpt\")\n",
    "            print(\"Model saved in file: %s\" % save_path)\n",
    "            \n",
    "    def predict(self, input_data):\n",
    "        saver = tf.train.Saver()\n",
    "        with  tf.Session() as sess:\n",
    "            # sess.run(self.init_op)\n",
    "            # Restore variables from disk.\n",
    "            saver.restore(sess, \"/jupyter/tomoaki/model.ckpt\")\n",
    "            print(\"Model restored.\")\n",
    "            output = self._output.eval(session=sess,\n",
    "                                                           feed_dict={self._input: input_data})\n",
    "        return output    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f1f640a3750>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f1f641c1710>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (228, 269) for Tensor u'Placeholder_1:0', which has shape '(?, 1)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-fa1084d65ed2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m lstm.training(input_train, target_train,\n\u001b[0;32m     17\u001b[0m                       \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecay_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecay_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m                       n_batch=n_batch, n_epochs=1000)\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[0melapsed_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mst_s\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"learning_time:\"\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0melapsed_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-47-78e2c288e4a6>\u001b[0m in \u001b[0;36mtraining\u001b[1;34m(self, input_data, target_data, n_batch, n_epochs, learning_rate, decay_rate, decay_freq)\u001b[0m\n\u001b[0;32m    103\u001b[0m                 sess.run(self.train, \n\u001b[0;32m    104\u001b[0m                                 feed_dict={self._input: input_data,\n\u001b[1;32m--> 105\u001b[1;33m                                                      self._target: target_data})\n\u001b[0m\u001b[0;32m    106\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdecay_freq\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m                     \u001b[0mlearning_rate\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mdecay_rate\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    370\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 372\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    373\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    623\u001b[0m                 \u001b[1;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m                 \u001b[1;34m'which has shape %r'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[0;32m    626\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tensor %s may not be fed.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot feed value of shape (228, 269) for Tensor u'Placeholder_1:0', which has shape '(?, 1)'"
     ]
    }
   ],
   "source": [
    "N = int(len(df_input) * 0.5)\n",
    "\n",
    "input_train = df_input[:N]\n",
    "target_train = df_target[:N]\n",
    "input_pred = df_input[N:]\n",
    "target_pred = df_target[N:]\n",
    "n_in = len(input_train[0])\n",
    "n_hidden1 = int(0.3 * n_in)\n",
    "n_out = 1\n",
    "n_batch=20\n",
    "\n",
    "lstm = BNLSTM(layers=[n_in, n_hidden1, n_out], is_initialize=True)\n",
    "\n",
    "print (\"start!\")\n",
    "st_s = time.time()\n",
    "lstm.training(input_train, target_train,\n",
    "                      learning_rate=0.1, decay_freq=100, decay_rate=0.5,\n",
    "                      n_batch=n_batch, n_epochs=1000)\n",
    "elapsed_s = time.time() - st_s\n",
    "print (\"learning_time:\",  elapsed_s)\n",
    "\n",
    "pred = mlp.predict(input_pred)\n",
    "plt.plot(pred, label=\"predit\")\n",
    "plt.plot(target_pred, label=\"raw data\")\n",
    "plt.ylim(min(target_pred), max(target_pred))\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
